{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T08:17:28.130051Z",
     "start_time": "2025-08-22T08:17:26.160107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from envs import TicTacToeTrainingEnv\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "from utils.terminal_colors import *\n",
    "from utils.json_utils import save_opponent_stats, load_opponent_stats\n",
    "from utils.agents_utils import should_save_agent, get_agents, get_last_agent_number\n",
    "from utils.evaluator import evaluate_agent_by_opponent\n",
    "from utils.visualize import defeat_rate_plot\n",
    "from training.config import *\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from utils.action_mask_ import mask_fn\n",
    "import json"
   ],
   "id": "48163c52afbd6247",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1 style=\"color:#0b9ed8\">TRAINING</h1>",
   "id": "54afbc8fa5be6463"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T08:17:28.148643Z",
     "start_time": "2025-08-22T08:17:28.141043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_env(opponent_pool):\n",
    "    \"\"\"\n",
    "    Create and wrap the TicTacToe training environment once per training session.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    opponent_pool : list of opponent names or agents against which the agent will train.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    env : ActionMasker\n",
    "        Wrapped TicTacToe environment ready for training.\n",
    "    \"\"\"\n",
    "    env_init = TicTacToeTrainingEnv(\n",
    "        board_length=TRAINING_DEFAULT_BOARD_LENGTH,\n",
    "        pattern_victory_length=TRAINING_DEFAULT_PATTERN_VICTORY_LENGTH,\n",
    "        opponent_pool=opponent_pool,\n",
    "        first_play_rate=TRAINING_DEFAULT_FIRST_PLAY_RATE,\n",
    "        lost_games_path=DEFEAT_PATH,\n",
    "        review_ratio=TRAINING_DEFAULT_REVIEW_RATIO,\n",
    "        opponent_statistics_file=BEST_STATS_PATH,\n",
    "    )\n",
    "    env = ActionMasker(env_init, mask_fn)\n",
    "    env.reset()\n",
    "    return env"
   ],
   "id": "1cd234e3712da8db",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T08:17:28.304304Z",
     "start_time": "2025-08-22T08:17:28.296843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_agent(env, last_agent_num, ent_coef, n_steps, batch_size, learning_rate):\n",
    "    \"\"\"\n",
    "    Initialize or load the PPO agent. Updates dynamic training parameters if agent exists.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    env : ActionMasker\n",
    "        The training environment.\n",
    "    last_agent_num : int\n",
    "        The number of the last saved agent.\n",
    "    ent_coef : float\n",
    "        Entropy coefficient for exploration.\n",
    "    n_steps : int\n",
    "        Number of steps to run for each environment per update.\n",
    "    batch_size : int\n",
    "        Size of minibatches for training.\n",
    "    learning_rate : float\n",
    "        Learning rate for training.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    agent : MaskablePPO\n",
    "        Initialized or loaded PPO agent.\n",
    "    \"\"\"\n",
    "    checkpoint_path = os.path.join(AGENTS_DIR, \"last_checkpoint.zip\")\n",
    "\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        agent = MaskablePPO.load(checkpoint_path, env=env)\n",
    "        print(\"âœ… Loaded agent from last checkpoint.\")\n",
    "    elif last_agent_num == 0:\n",
    "        agent = MaskablePPO(\n",
    "            \"MultiInputPolicy\",\n",
    "            env=env,\n",
    "            verbose=1,\n",
    "            gamma=GAMMA,\n",
    "            gae_lambda=GAE_LAMBDA,\n",
    "            ent_coef=ent_coef,\n",
    "            n_steps=n_steps,\n",
    "            batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            policy_kwargs=policy_kwargs\n",
    "        )\n",
    "    else:\n",
    "        prev_agent_path = get_agents(AGENTS_DIR)[-1]\n",
    "        agent = MaskablePPO.load(prev_agent_path, env=env)\n",
    "\n",
    "    # Update dynamic parameters\n",
    "    agent.ent_coef = ent_coef\n",
    "    agent.n_steps = n_steps\n",
    "    agent.batch_size = batch_size\n",
    "    agent.learning_rate = learning_rate\n",
    "    return agent"
   ],
   "id": "7b2dafd047651823",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T08:17:28.354701Z",
     "start_time": "2025-08-22T08:17:28.331935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_one_agent():\n",
    "    \"\"\"\n",
    "    Train a single PPO agent, evaluate against opponents, and save stats continuously.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    improvement : bool\n",
    "        True if agent shows improvement over previous best.\n",
    "    \"\"\"\n",
    "    last_agent_num = get_last_agent_number(AGENTS_DIR)\n",
    "    next_agent_num = last_agent_num + 1\n",
    "    agent_name = f\"agent_v{next_agent_num}_{BASE_AGENTS_NAME}.zip\"\n",
    "    agent_path = os.path.join(AGENTS_DIR, agent_name)\n",
    "\n",
    "    opponent_agents = get_agents(AGENTS_DIR)\n",
    "    opponent_pool = [\"random\", \"smart_random\"] + opponent_agents\n",
    "    improvement = False\n",
    "    best_stats = load_opponent_stats(opponent_pool)\n",
    "    n_checks = TOTAL_STEPS // CHECKPOINT_INTERVAL\n",
    "\n",
    "    env = create_env(opponent_pool)\n",
    "\n",
    "    for check in range(n_checks):\n",
    "        current_progress = (check * CHECKPOINT_INTERVAL) / TOTAL_STEPS\n",
    "\n",
    "        # Dynamic training parameters\n",
    "        n_steps = int(2048 + (4096 - 2048) * current_progress**0.8)\n",
    "        batch_size = min(1024, int(512 + (2048 - 512) * current_progress**1.0))\n",
    "        ent_coef = 0.01\n",
    "        learning_rate = LR_SCHEDULE(current_progress)\n",
    "\n",
    "        if check == 0:\n",
    "            agent = initialize_agent(env, last_agent_num, ent_coef, n_steps, batch_size, learning_rate)\n",
    "\n",
    "        print(f\"\\n{YELLOW}=== Training segment {check+1}/{n_checks} ===\")\n",
    "        print(f\"Steps: {check*CHECKPOINT_INTERVAL}-{(check+1)*CHECKPOINT_INTERVAL}\")\n",
    "        print(f\"Params: n_steps={n_steps}, batch={batch_size}, ent_coef={ent_coef:.4f}\")\n",
    "        print(f\"Opponents: {opponent_pool}{RESET}\\n\")\n",
    "\n",
    "        # Train agent\n",
    "        agent.learn(total_timesteps=CHECKPOINT_INTERVAL)\n",
    "\n",
    "        # Evaluate agent\n",
    "        results = evaluate_agent_by_opponent(agent, opponent_pool, n_episodes=2000)\n",
    "        current_stats = {k: {\"defeat_rate\": v[\"defeat_rate\"], \"victory_rate\": v[\"victory_rate\"]} for k, v in results.items()}\n",
    "\n",
    "        # Save improvement if criteria met\n",
    "        if should_save_agent(current_stats, best_stats, IMPROVEMENT_THRESHOLD):\n",
    "            print(f\"{GREEN}Saved new best agent at checkpoint {check}{RESET}\")\n",
    "            improvement = True\n",
    "            best_stats = deepcopy(current_stats)\n",
    "            agent.save(agent_path)\n",
    "            save_opponent_stats(best_stats, BEST_STATS_PATH)\n",
    "\n",
    "        # Save all stats to JSON file continuously\n",
    "        all_stats_data = {}\n",
    "        if os.path.exists(ALL_STATS_PATH):\n",
    "            with open(ALL_STATS_PATH, \"r\") as f:\n",
    "                all_stats_data = json.load(f)\n",
    "\n",
    "        # Determine the next checkpoint index based on existing entries\n",
    "        next_checkpoint = len(all_stats_data) + 1\n",
    "\n",
    "        # Store current evaluation for this checkpoint\n",
    "        all_stats_data[f\"checkpoint_{next_checkpoint}\"] = {\n",
    "            opp: {\n",
    "                \"overall_defeat_rate\": results[opp][\"defeat_rate\"],\n",
    "                \"first_player_defeat_rate\": results[opp][\"losses_play_first\"] / 1000,\n",
    "                \"second_player_defeat_rate\": results[opp][\"losses_play_second\"] / 1000 ,\n",
    "            }\n",
    "            for opp in opponent_pool\n",
    "        }\n",
    "\n",
    "\n",
    "        with open(ALL_STATS_PATH, \"w\") as f:\n",
    "            json.dump(all_stats_data, f, indent=4)\n",
    "\n",
    "        # Early stopping if all defeat rates are zero\n",
    "        all_defeat_zero = all(stats[\"defeat_rate\"] == 0.0 for stats in current_stats.values())\n",
    "        if all_defeat_zero:\n",
    "            print(f\"{GREEN}=== All defeat rates are 0. Early stopping triggered. ==={RESET}\")\n",
    "            break\n",
    "\n",
    "    agent.save(os.path.join(AGENTS_DIR, \"last_checkpoint.zip\"))\n",
    "    return improvement"
   ],
   "id": "691d8eaa0a5531b6",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T08:17:28.405032Z",
     "start_time": "2025-08-22T08:17:28.395186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main_training_loop(nb_agents_to_train=5):\n",
    "    \"\"\"\n",
    "    Main training loop to train multiple PPO agents sequentially.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    nb_agents_to_train : int\n",
    "        Number of agents to train in this session.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    trained_count = 0\n",
    "    while trained_count < nb_agents_to_train:\n",
    "        improvement = train_one_agent()  # now returns only improvement\n",
    "        if improvement:\n",
    "            print(f\"{GREEN}Training completed for agent {trained_count+1}. Best agent saved.{RESET}\")\n",
    "        else:\n",
    "            print(f\"{RED}Warning: No agent met improvement criteria{RESET}\")\n",
    "        trained_count += 1  # continue to next agent regardless\n"
   ],
   "id": "b5745c7fe5ea057d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T08:17:29.950477Z",
     "start_time": "2025-08-22T08:17:28.456939Z"
    }
   },
   "cell_type": "code",
   "source": [
    " # Run the training loop\n",
    "main_training_loop(2)"
   ],
   "id": "be49845952d90eff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "\n",
      "\u001B[33m=== Training segment 1/10 ===\n",
      "Steps: 0-10000\n",
      "Params: n_steps=2048, batch=512, ent_coef=0.0100\n",
      "Opponents: ['random', 'smart_random']\u001B[0m\n",
      "\n",
      "--- agent play in 1 position and chose action 7 ---\n",
      "array([[1, 0, 1],\n",
      "       [3, 3, 0],\n",
      "       [0, 1, 3]], dtype=int8)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Run the training loop\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mmain_training_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[5], line 16\u001B[0m, in \u001B[0;36mmain_training_loop\u001B[0;34m(nb_agents_to_train)\u001B[0m\n\u001B[1;32m     14\u001B[0m trained_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m trained_count \u001B[38;5;241m<\u001B[39m nb_agents_to_train:\n\u001B[0;32m---> 16\u001B[0m     improvement \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_one_agent\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# now returns only improvement\u001B[39;00m\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m improvement:\n\u001B[1;32m     18\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mGREEN\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mTraining completed for agent \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrained_count\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Best agent saved.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mRESET\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[4], line 41\u001B[0m, in \u001B[0;36mtrain_one_agent\u001B[0;34m()\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOpponents: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mopponent_pool\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mRESET\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Train agent\u001B[39;00m\n\u001B[0;32m---> 41\u001B[0m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mCHECKPOINT_INTERVAL\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;66;03m# Evaluate agent\u001B[39;00m\n\u001B[1;32m     44\u001B[0m results \u001B[38;5;241m=\u001B[39m evaluate_agent_by_opponent(agent, opponent_pool, n_episodes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2000\u001B[39m)\n",
      "File \u001B[0;32m~/PROGRAMMATION/PYTHON/PERSO/EXERCISM/venv/lib/python3.10/site-packages/sb3_contrib/ppo_mask/ppo_mask.py:454\u001B[0m, in \u001B[0;36mMaskablePPO.learn\u001B[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, use_masking, progress_bar)\u001B[0m\n\u001B[1;32m    451\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    453\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps \u001B[38;5;241m<\u001B[39m total_timesteps:\n\u001B[0;32m--> 454\u001B[0m     continue_training \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect_rollouts\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrollout_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_steps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_masking\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    456\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m continue_training:\n\u001B[1;32m    457\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/PROGRAMMATION/PYTHON/PERSO/EXERCISM/venv/lib/python3.10/site-packages/sb3_contrib/ppo_mask/ppo_mask.py:233\u001B[0m, in \u001B[0;36mMaskablePPO.collect_rollouts\u001B[0;34m(self, env, callback, rollout_buffer, n_rollout_steps, use_masking)\u001B[0m\n\u001B[1;32m    230\u001B[0m     actions, values, log_probs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy(obs_tensor, action_masks\u001B[38;5;241m=\u001B[39maction_masks)\n\u001B[1;32m    232\u001B[0m actions \u001B[38;5;241m=\u001B[39m actions\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m--> 233\u001B[0m new_obs, rewards, dones, infos \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mactions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mnum_envs\n\u001B[1;32m    237\u001B[0m \u001B[38;5;66;03m# Give access to local variables\u001B[39;00m\n",
      "File \u001B[0;32m~/PROGRAMMATION/PYTHON/PERSO/EXERCISM/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:222\u001B[0m, in \u001B[0;36mVecEnv.step\u001B[0;34m(self, actions)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    216\u001B[0m \u001B[38;5;124;03mStep the environments with the given action\u001B[39;00m\n\u001B[1;32m    217\u001B[0m \n\u001B[1;32m    218\u001B[0m \u001B[38;5;124;03m:param actions: the action\u001B[39;00m\n\u001B[1;32m    219\u001B[0m \u001B[38;5;124;03m:return: observation, reward, done, information\u001B[39;00m\n\u001B[1;32m    220\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    221\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep_async(actions)\n\u001B[0;32m--> 222\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep_wait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PROGRAMMATION/PYTHON/PERSO/EXERCISM/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:59\u001B[0m, in \u001B[0;36mDummyVecEnv.step_wait\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mstep_wait\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m VecEnvStepReturn:\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;66;03m# Avoid circular imports\u001B[39;00m\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m env_idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_envs):\n\u001B[0;32m---> 59\u001B[0m         obs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_rews[env_idx], terminated, truncated, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_infos[env_idx] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menvs\u001B[49m\u001B[43m[\u001B[49m\u001B[43menv_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[assignment]\u001B[39;49;00m\n\u001B[1;32m     60\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mactions\u001B[49m\u001B[43m[\u001B[49m\u001B[43menv_idx\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     61\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     62\u001B[0m         \u001B[38;5;66;03m# convert to SB3 VecEnv api\u001B[39;00m\n\u001B[1;32m     63\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_dones[env_idx] \u001B[38;5;241m=\u001B[39m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated\n",
      "File \u001B[0;32m~/PROGRAMMATION/PYTHON/PERSO/EXERCISM/venv/lib/python3.10/site-packages/stable_baselines3/common/monitor.py:94\u001B[0m, in \u001B[0;36mMonitor.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mneeds_reset:\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTried to step environment that needs reset\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 94\u001B[0m observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrewards\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mfloat\u001B[39m(reward))\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated:\n",
      "File \u001B[0;32m~/PROGRAMMATION/PYTHON/PERSO/EXERCISM/venv/lib/python3.10/site-packages/gymnasium/core.py:327\u001B[0m, in \u001B[0;36mWrapper.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    323\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mstep\u001B[39m(\n\u001B[1;32m    324\u001B[0m     \u001B[38;5;28mself\u001B[39m, action: WrapperActType\n\u001B[1;32m    325\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[WrapperObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[1;32m    326\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 327\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PROGRAMMATION/AI/tic_tac_toe_rl/envs/training_env.py:257\u001B[0m, in \u001B[0;36mTicTacToeTrainingEnv.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnumber_turn \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    254\u001B[0m \u001B[38;5;66;03m# ---------------------\u001B[39;00m\n\u001B[1;32m    255\u001B[0m \u001B[38;5;66;03m# Agent's turn\u001B[39;00m\n\u001B[1;32m    256\u001B[0m \u001B[38;5;66;03m# ---------------------\u001B[39;00m\n\u001B[0;32m--> 257\u001B[0m obs_agent, reward_agent, done_agent, truncated_agent, info_agent \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluation:\n\u001B[1;32m    259\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magent_blows\u001B[38;5;241m.\u001B[39mappend(action)\n",
      "\u001B[0;31mValueError\u001B[0m: not enough values to unpack (expected 5, got 4)"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "defeat_rate_plot()",
   "id": "594c0b9e981ae998",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

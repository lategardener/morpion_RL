{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from envs import TicTacToeTrainingEnv\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "from utils.terminal_colors import *\n",
    "from utils.json_utils import save_opponent_stats, load_opponent_stats\n",
    "from utils.models_utils import should_save_model, get_models, get_last_model_number\n",
    "from utils.evaluator import evaluate_model_by_opponent\n",
    "from utils.visualize import defeat_rate_plot\n",
    "from training.config import *\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from test.action_mask_ import mask_fn\n",
    "import json"
   ],
   "id": "48163c52afbd6247",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1 style=\"color:#0b9ed8\">TRAINING</h1>",
   "id": "54afbc8fa5be6463"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_env(opponent_pool):\n",
    "    \"\"\"\n",
    "    Create and wrap the TicTacToe training environment once per training session.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    opponent_pool : list of opponent names or models against which the agent will train.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    env : ActionMasker\n",
    "        Wrapped TicTacToe environment ready for training.\n",
    "    \"\"\"\n",
    "    env_init = TicTacToeTrainingEnv(\n",
    "        board_length=TRAINING_DEFAULT_BOARD_LENGTH,\n",
    "        pattern_victory_length=TRAINING_DEFAULT_PATTERN_VICTORY_LENGTH,\n",
    "        opponent_pool=opponent_pool,\n",
    "        first_play_rate=TRAINING_DEFAULT_FIRST_PLAY_RATE,\n",
    "        lost_games_path=DEFEAT_PATH,\n",
    "        review_ratio=TRAINING_DEFAULT_REVIEW_RATIO,\n",
    "        opponent_statistics_file=BEST_STATS_PATH,\n",
    "    )\n",
    "    env = ActionMasker(env_init, mask_fn)\n",
    "    env.reset()\n",
    "    return env"
   ],
   "id": "1cd234e3712da8db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def initialize_model(env, last_model_num, ent_coef, n_steps, batch_size, learning_rate):\n",
    "    \"\"\"\n",
    "    Initialize or load the PPO model. Updates dynamic training parameters if model exists.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    env : ActionMasker\n",
    "        The training environment.\n",
    "    last_model_num : int\n",
    "        The number of the last saved model.\n",
    "    ent_coef : float\n",
    "        Entropy coefficient for exploration.\n",
    "    n_steps : int\n",
    "        Number of steps to run for each environment per update.\n",
    "    batch_size : int\n",
    "        Size of minibatches for training.\n",
    "    learning_rate : float\n",
    "        Learning rate for training.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    model : MaskablePPO\n",
    "        Initialized or loaded PPO model.\n",
    "    \"\"\"\n",
    "    checkpoint_path = os.path.join(MODELS_DIR, \"last_checkpoint.zip\")\n",
    "\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        model = MaskablePPO.load(checkpoint_path, env=env)\n",
    "        print(\"âœ… Loaded model from last checkpoint.\")\n",
    "    elif last_model_num == 0:\n",
    "        model = MaskablePPO(\n",
    "            \"MultiInputPolicy\",\n",
    "            env=env,\n",
    "            verbose=1,\n",
    "            gamma=GAMMA,\n",
    "            gae_lambda=GAE_LAMBDA,\n",
    "            ent_coef=ent_coef,\n",
    "            n_steps=n_steps,\n",
    "            batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            policy_kwargs=policy_kwargs\n",
    "        )\n",
    "    else:\n",
    "        prev_model_path = get_models(MODELS_DIR)[-1]\n",
    "        model = MaskablePPO.load(prev_model_path, env=env)\n",
    "\n",
    "    # Update dynamic parameters\n",
    "    model.ent_coef = ent_coef\n",
    "    model.n_steps = n_steps\n",
    "    model.batch_size = batch_size\n",
    "    model.learning_rate = learning_rate\n",
    "    return model"
   ],
   "id": "505a200d48e06172",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_one_model():\n",
    "    \"\"\"\n",
    "    Train a single PPO model, evaluate against opponents, and save stats continuously.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    improvement : bool\n",
    "        True if model shows improvement over previous best.\n",
    "    \"\"\"\n",
    "    last_model_num = get_last_model_number(MODELS_DIR)\n",
    "    next_model_num = last_model_num + 1\n",
    "    model_name = f\"{BASE_MODELS_NAME}_{next_model_num}.zip\"\n",
    "    model_path = os.path.join(MODELS_DIR, model_name)\n",
    "\n",
    "    opponent_models = get_models(MODELS_DIR)\n",
    "    opponent_pool = [\"random\", \"smart_random\"] + opponent_models\n",
    "    improvement = False\n",
    "    best_stats = load_opponent_stats(opponent_pool)\n",
    "    n_checks = TOTAL_STEPS // CHECKPOINT_INTERVAL\n",
    "\n",
    "    env = create_env(opponent_pool)\n",
    "\n",
    "    for check in range(n_checks):\n",
    "        current_progress = (check * CHECKPOINT_INTERVAL) / TOTAL_STEPS\n",
    "\n",
    "        # Dynamic training parameters\n",
    "        n_steps = int(2048 + (4096 - 2048) * current_progress**0.8)\n",
    "        batch_size = min(1024, int(512 + (2048 - 512) * current_progress**1.0))\n",
    "        ent_coef = 0.001\n",
    "        learning_rate = LR_SCHEDULE(current_progress)\n",
    "\n",
    "        if check == 0:\n",
    "            model = initialize_model(env, last_model_num, ent_coef, n_steps, batch_size, learning_rate)\n",
    "\n",
    "        print(f\"\\n{YELLOW}=== Training segment {check+1}/{n_checks} ===\")\n",
    "        print(f\"Steps: {check*CHECKPOINT_INTERVAL}-{(check+1)*CHECKPOINT_INTERVAL}\")\n",
    "        print(f\"Params: n_steps={n_steps}, batch={batch_size}, ent_coef={ent_coef:.4f}\")\n",
    "        print(f\"Opponents: {opponent_pool}{RESET}\\n\")\n",
    "\n",
    "        # Train model\n",
    "        model.learn(total_timesteps=CHECKPOINT_INTERVAL)\n",
    "\n",
    "        # Evaluate model\n",
    "        results = evaluate_model_by_opponent(model, opponent_pool, n_episodes=2000)\n",
    "        current_stats = {k: {\"defeat_rate\": v[\"defeat_rate\"], \"victory_rate\": v[\"victory_rate\"]} for k, v in results.items()}\n",
    "\n",
    "        # Save improvement if criteria met\n",
    "        if should_save_model(current_stats, best_stats, IMPROVEMENT_THRESHOLD):\n",
    "            print(f\"{GREEN}Saved new best model at checkpoint {check}{RESET}\")\n",
    "            improvement = True\n",
    "            best_stats = deepcopy(current_stats)\n",
    "            model.save(model_path)\n",
    "            save_opponent_stats(best_stats, BEST_STATS_PATH)\n",
    "\n",
    "        # Save all stats to JSON file continuously\n",
    "        all_stats_data = {}\n",
    "        if os.path.exists(ALL_STATS_PATH):\n",
    "            with open(ALL_STATS_PATH, \"r\") as f:\n",
    "                all_stats_data = json.load(f)\n",
    "\n",
    "        # Determine the next checkpoint index based on existing entries\n",
    "        next_checkpoint = len(all_stats_data) + 1\n",
    "\n",
    "        # Store current evaluation for this checkpoint\n",
    "        all_stats_data[f\"checkpoint_{next_checkpoint}\"] = {\n",
    "            opp: {\n",
    "                \"overall_defeat_rate\": results[opp][\"defeat_rate\"],\n",
    "                \"first_player_defeat_rate\": results[opp][\"losses_play_first\"] / 1000,\n",
    "                \"second_player_defeat_rate\": results[opp][\"losses_play_second\"] / 1000,\n",
    "            }\n",
    "            for opp in opponent_pool\n",
    "        }\n",
    "\n",
    "\n",
    "        with open(ALL_STATS_PATH, \"w\") as f:\n",
    "            json.dump(all_stats_data, f, indent=4)\n",
    "\n",
    "        # Early stopping if all defeat rates are zero\n",
    "        all_defeat_zero = all(stats[\"defeat_rate\"] == 0.0 for stats in current_stats.values())\n",
    "        if all_defeat_zero:\n",
    "            print(f\"{GREEN}=== All defeat rates are 0. Early stopping triggered. ==={RESET}\")\n",
    "            break\n",
    "\n",
    "    model.save(os.path.join(MODELS_DIR, \"last_checkpoint.zip\"))\n",
    "    return improvement"
   ],
   "id": "345f71fefcf28b3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def main_training_loop(nb_models_to_train=2):\n",
    "    \"\"\"\n",
    "    Main training loop to train multiple PPO models sequentially.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    nb_models_to_train : int\n",
    "        Number of models to train in this session.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    trained_count = 0\n",
    "    while trained_count < nb_models_to_train:\n",
    "        improvement = train_one_model()  # now returns only improvement\n",
    "        if improvement:\n",
    "            print(f\"{GREEN}Training completed for model {trained_count+1}. Best model saved.{RESET}\")\n",
    "        else:\n",
    "            print(f\"{RED}Warning: No model met improvement criteria{RESET}\")\n",
    "        trained_count += 1  # continue to next model regardless\n"
   ],
   "id": "c65dd6510ed9b181",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " # Run the training loop\n",
    "main_training_loop(2)"
   ],
   "id": "361448dc0742ab2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "defeat_rate_plot()",
   "id": "594c0b9e981ae998",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

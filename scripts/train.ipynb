{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from envs import TicTacToeBaseEnv, TicTacToeTrainingEnv\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import json\n",
    "import numpy as np\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "import wandb\n",
    "from scripts.action_mask_ import mask_fn\n",
    "from sb3_contrib import MaskablePPO"
   ],
   "id": "303d68e3ed027acc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1 style=\"color:red\">REAL TRAINING</h1>",
   "id": "54afbc8fa5be6463"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# üìÅ Chemin vers le fichier JSON des taux de d√©faite\n",
    "models_dir = \"MODELS_9x9\"\n",
    "DEFEAT_RATE_PATH = os.path.join(models_dir, \"defeat_rates.json\")\n",
    "\n",
    "def convert_to_serializable(obj):\n",
    "    \"\"\"Convertit les objets non s√©rialisables (comme ndarray) en types compatibles JSON.\"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (np.int64, np.int32)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.float64, np.float32)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(item) for item in obj]\n",
    "    return obj\n",
    "\n",
    "\n",
    "def load_defeat_rates(opponents):\n",
    "    if os.path.exists(DEFEAT_RATE_PATH):\n",
    "        with open(DEFEAT_RATE_PATH, \"r\") as f:\n",
    "            saved_rates = json.load(f)\n",
    "    else:\n",
    "        saved_rates = {}\n",
    "\n",
    "    for opp in opponents:\n",
    "        if opp not in saved_rates:\n",
    "            saved_rates[opp] = 1.0\n",
    "\n",
    "    return saved_rates\n",
    "\n",
    "def save_defeat_rates(rates):\n",
    "    with open(DEFEAT_RATE_PATH, \"w\") as f:\n",
    "        json.dump(rates, f, indent=4)\n",
    "\n",
    "def evaluate_model_by_opponent(model, opponent_pool, n_episodes=100):\n",
    "    global reward\n",
    "    results = {}\n",
    "    defeated_games = {}\n",
    "    all_boards = []\n",
    "\n",
    "    for opponent in opponent_pool:\n",
    "        env = TicTacToeTrainingEnv(board_length=9,\n",
    "                                   pattern_victory_length=5,\n",
    "                                   opponent_pool=[opponent],\n",
    "                                   evaluation=True,\n",
    "                                   first_play_rate=0.2,\n",
    "                                   lost_games_path=\"defeated_games.json\",\n",
    "                                   review_ratio=0.0)\n",
    "\n",
    "        wins_play_first, losses_play_first, draws_play_first = 0, 0, 0\n",
    "        wins_play_second, losses_play_second, draws_play_second = 0, 0, 0\n",
    "\n",
    "        defeat_count = 0\n",
    "        for _ in range(n_episodes):\n",
    "            obs, _ = env.reset()\n",
    "            done = False\n",
    "            episode_data = {\"player\" : env.player, \"board\": []}\n",
    "\n",
    "            while not done:\n",
    "                action, _ = model.predict(obs, deterministic=True, action_masks=obs[\"action_mask\"])\n",
    "                obs, reward, done, _, _ = env.step(action)\n",
    "                if not done:\n",
    "                    episode_data[\"board\"] = convert_to_serializable(obs[\"observation\"])\n",
    "\n",
    "            if reward == env.victory_reward:\n",
    "                if env.first_to_play:\n",
    "                    wins_play_first += 1\n",
    "                else:\n",
    "                    wins_play_second += 1\n",
    "            elif reward == -env.victory_reward:\n",
    "                defeat_count += 1\n",
    "                if episode_data[\"board\"] not in all_boards:\n",
    "                    all_boards.append(episode_data[\"board\"])\n",
    "                    defeated_games[f\"game_lose_against_{opponent}_{defeat_count}\"] = episode_data\n",
    "                if env.first_to_play:\n",
    "                    losses_play_first += 1\n",
    "                else:\n",
    "                    losses_play_second += 1\n",
    "            else:\n",
    "                if env.first_to_play:\n",
    "                    draws_play_first += 1\n",
    "                else:\n",
    "                    draws_play_second += 1\n",
    "\n",
    "        results[opponent] = {\n",
    "            \"wins_play_first\": wins_play_first,\n",
    "            \"wins_play_second\": wins_play_second,\n",
    "            \"losses_play_first\": losses_play_first,\n",
    "            \"losses_play_second\": losses_play_second,\n",
    "            \"draws_play_first\": draws_play_first,\n",
    "            \"draws_play_second\": draws_play_second,\n",
    "            \"defeat_rate\": (losses_play_first + losses_play_second) / n_episodes\n",
    "        }\n",
    "\n",
    "    if defeated_games:\n",
    "        with open(\"defeated_games.json\", \"w\") as f:\n",
    "            json.dump(convert_to_serializable(defeated_games), f, indent=4)\n",
    "    return results\n",
    "\n",
    "def linear_schedule(start_value):\n",
    "    def schedule(progress):\n",
    "        return start_value * progress\n",
    "    return schedule\n",
    "\n",
    "# üîß Hyperparam√®tres\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "START_ENT_COEF = 0.01\n",
    "\n",
    "start_model_index = 2\n",
    "max_models = 2\n",
    "\n",
    "for i in range(start_model_index, max_models + 1):\n",
    "    model_name = f\"ppo_tictactoe_{i}.zip\"\n",
    "    model_path = os.path.join(models_dir, model_name)\n",
    "\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "    def get_models(path):\n",
    "        return sorted(\n",
    "            [os.path.join(path, f) for f in os.listdir(path) if f.startswith(\"ppo_tictactoe_\") and f.endswith(\".zip\")],\n",
    "            key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0])\n",
    "        )\n",
    "\n",
    "    opponent_models = get_models(models_dir)\n",
    "    opponent_pool = [\"random\"] * 4 + [\"smart_random\"] * 1 + opponent_pool * 1\n",
    "\n",
    "    defeat_rates = load_defeat_rates(opponent_pool)\n",
    "\n",
    "    env_init = TicTacToeTrainingEnv(board_length=9,\n",
    "                                    pattern_victory_length=5,\n",
    "                                    opponent_pool=opponent_pool,\n",
    "                                    first_play_rate=0.4,\n",
    "                                    lost_games_path=\"defeated_games.json\",\n",
    "                                    review_ratio=0.0)\n",
    "\n",
    "    env = ActionMasker(env_init, mask_fn)\n",
    "\n",
    "    wandb.init(\n",
    "        project=f\"{models_dir}_ppo-tictactoe\",\n",
    "        name=f\"{models_dir}-run_model_{i}\",\n",
    "        config={\n",
    "            \"model_index\": i,\n",
    "            \"opponents\": opponent_pool,\n",
    "            \"current_defeat_rates\" : defeat_rates,\n",
    "            \"gamma\": GAMMA,\n",
    "            \"gae_lambda\": GAE_LAMBDA,\n",
    "            \"ent_coef_start\": START_ENT_COEF\n",
    "        },\n",
    "        reinit=True\n",
    "    )\n",
    "\n",
    "    if i == 1:\n",
    "        model = MaskablePPO(\n",
    "            \"MultiInputPolicy\",\n",
    "            env,\n",
    "            verbose=1,\n",
    "            gamma=GAMMA,\n",
    "            gae_lambda=GAE_LAMBDA,\n",
    "            ent_coef=START_ENT_COEF,\n",
    "        )\n",
    "    else:\n",
    "        prev_model_path = get_models(models_dir)[-1]\n",
    "        print(f\"üîÑ Chargement du mod√®le pr√©c√©dent : {prev_model_path}\")\n",
    "        model = MaskablePPO.load(\n",
    "            prev_model_path,\n",
    "            env=env,\n",
    "            gamma=GAMMA,\n",
    "            gae_lambda=GAE_LAMBDA,\n",
    "            ent_coef=START_ENT_COEF,\n",
    "        )\n",
    "\n",
    "    total_steps = 50_000\n",
    "    patience = 1\n",
    "    no_improve_counter = 0\n",
    "\n",
    "    while True:\n",
    "        print(f\"üéØ Entra√Ænement mod√®le {i} ({models_dir}) contre ({opponent_pool}) pour {total_steps} steps...\")\n",
    "        model.learn(total_timesteps=total_steps)\n",
    "\n",
    "        results = evaluate_model_by_opponent(model, opponent_pool)\n",
    "\n",
    "        current_defeat_rates = {k: v[\"defeat_rate\"] for k, v in results.items()}\n",
    "        wandb.log({f\"{k}_defeat_rate\": v for k, v in current_defeat_rates.items()})\n",
    "\n",
    "        improved_or_equal = all(current_defeat_rates[k] <= defeat_rates.get(k, 1.0) for k in current_defeat_rates)\n",
    "\n",
    "        if improved_or_equal:\n",
    "            print(\"‚úÖ Taux de d√©faite r√©duit ou constant pour tous les adversaires. Sauvegarde du mod√®le.\")\n",
    "            model.save(model_path)\n",
    "            defeat_rates.update(current_defeat_rates)\n",
    "            save_defeat_rates(defeat_rates)\n",
    "            break\n",
    "        else:\n",
    "            no_improve_counter += 1\n",
    "            print(f\"‚è≥ Pas d'am√©lioration sur tous les adversaires ({no_improve_counter}/{patience})\")\n",
    "\n",
    "            if no_improve_counter >= patience:\n",
    "                print(\"üõë Arr√™t de l'entra√Ænement : pas d'am√©lioration suffisante.\")\n",
    "                break\n",
    "\n",
    "        wandb.log({\n",
    "            **{f\"{k}_wins_play_first\": v[\"wins_play_first\"] for k, v in results.items()},\n",
    "            **{f\"{k}_wins_play_second\": v[\"wins_play_second\"] for k, v in results.items()},\n",
    "            **{f\"{k}_draws_play_first\": v[\"draws_play_first\"] for k, v in results.items()},\n",
    "            **{f\"{k}_draws_play_second\": v[\"draws_play_second\"] for k, v in results.items()},\n",
    "            **{f\"{k}_losses_play_first\": v[\"losses_play_first\"] for k, v in results.items()},\n",
    "            **{f\"{k}_losses_play_second\": v[\"losses_play_second\"] for k, v in results.items()},\n",
    "        })\n",
    "\n",
    "    model.save(model_path)\n",
    "    print(f\"‚úÖ Mod√®le {i} sauvegard√© dans {model_path}\")\n",
    "\n",
    "    example_result = next(iter(results.values()))\n",
    "    with open(os.path.join(models_dir, \"training_log.txt\"), \"a\") as log_file:\n",
    "        log_file.write(\n",
    "            f\"Mod√®le {i}: {example_result['wins_play_first']}W_1st - {example_result['losses_play_first']}L_1st - \"\n",
    "            f\"{example_result['draws_play_first']}D_1st - {example_result['wins_play_second']}W_2nd - \"\n",
    "            f\"{example_result['losses_play_second']}L_2nd - {example_result['draws_play_second']}D_2nd | \"\n",
    "            f\"Max Defeat Rate: {max(current_defeat_rates.values()):.2f}\\n\"\n",
    "        )\n",
    "\n",
    "\n",
    "    wandb.finish()\n"
   ],
   "id": "e7f729d70320b19"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

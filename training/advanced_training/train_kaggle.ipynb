{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T08:31:57.513669Z",
     "start_time": "2025-08-14T08:31:55.731610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from envs import TicTacToeTrainingEnv\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "from utils.terminal_colors import *\n",
    "from utils.json_utils import save_opponent_stats, load_opponent_stats\n",
    "from utils.models_utils import should_save_model, get_models, get_last_model_number\n",
    "from utils.evaluator import evaluate_model_by_opponent\n",
    "from training.advanced_training.config import *\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from test.action_mask_ import mask_fn"
   ],
   "id": "303d68e3ed027acc",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1 style=\"color:#0b9ed8\">TRAINING</h1>",
   "id": "54afbc8fa5be6463"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T08:31:57.525107Z",
     "start_time": "2025-08-14T08:31:57.521063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_env(opponent_pool):\n",
    "    \"\"\"Create and wrap the training environment once per training session.\"\"\"\n",
    "    env_init = TicTacToeTrainingEnv(\n",
    "        board_length=TRAINING_DEFAULT_BOARD_LENGTH,\n",
    "        pattern_victory_length=TRAINING_DEFAULT_PATTERN_VICTORY_LENGTH,\n",
    "        opponent_pool=opponent_pool,\n",
    "        first_play_rate=0.3,\n",
    "        lost_games_path=DEFEAT_PATH,\n",
    "        review_ratio=0.0,\n",
    "        opponent_statistics_file=STATS_PATH,\n",
    "    )\n",
    "    env = ActionMasker(env_init, mask_fn)\n",
    "    env.reset()\n",
    "    return env"
   ],
   "id": "1cd234e3712da8db",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T08:31:57.653073Z",
     "start_time": "2025-08-14T08:31:57.647951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_model(env, last_model_num, ent_coef, n_steps, batch_size, learning_rate):\n",
    "    checkpoint_path = os.path.join(MODELS_DIR, \"last_checkpoint.zip\")\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        model = MaskablePPO.load(checkpoint_path, env=env)\n",
    "        print(\"✅ Loaded model from last checkpoint.\")\n",
    "    elif last_model_num == 0:\n",
    "        model = MaskablePPO(\n",
    "            \"MultiInputPolicy\",\n",
    "            env=env,\n",
    "            verbose=1,\n",
    "            gamma=GAMMA,\n",
    "            gae_lambda=GAE_LAMBDA,\n",
    "            ent_coef=ent_coef,\n",
    "            n_steps=n_steps,\n",
    "            batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            policy_kwargs=policy_kwargs\n",
    "        )\n",
    "    else:\n",
    "        prev_model_path = get_models(MODELS_DIR)[-1]\n",
    "        model = MaskablePPO.load(prev_model_path, env=env)\n",
    "\n",
    "    # Mettre à jour les paramètres dynamiques\n",
    "    model.ent_coef = ent_coef\n",
    "    model.n_steps = n_steps\n",
    "    model.batch_size = batch_size\n",
    "    model.learning_rate = learning_rate\n",
    "    return model\n"
   ],
   "id": "505a200d48e06172",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T08:31:57.706453Z",
     "start_time": "2025-08-14T08:31:57.694036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_one_model():\n",
    "    last_model_num = get_last_model_number(MODELS_DIR)\n",
    "    next_model_num = last_model_num + 1\n",
    "    model_name = f\"{BASE_MODELS_NAME}_{next_model_num}.zip\"\n",
    "    model_path = os.path.join(MODELS_DIR, model_name)\n",
    "\n",
    "    opponent_models = get_models(MODELS_DIR)\n",
    "    opponent_pool = [\"random\"] + opponent_models\n",
    "    improvement = False\n",
    "    best_stats = load_opponent_stats(opponent_pool)\n",
    "    n_checks = TOTAL_STEPS // CHECKPOINT_INTERVAL\n",
    "\n",
    "    # Create environment once\n",
    "    env = create_env(opponent_pool)\n",
    "\n",
    "    for check in range(n_checks):\n",
    "        current_progress = (check * CHECKPOINT_INTERVAL) / TOTAL_STEPS\n",
    "\n",
    "        # Dynamic training parameters\n",
    "        n_steps = int(2048 + (4096 - 2048) * current_progress**0.8)\n",
    "        batch_size = min(1024, int(512 + (2048 - 512) * current_progress**1.0))\n",
    "        ent_coef = 0.02\n",
    "        learning_rate = LR_SCHEDULE(current_progress)\n",
    "\n",
    "        # Initialize or load model only on first checkpoint\n",
    "        if check == 0:\n",
    "            model = initialize_model(env, last_model_num, ent_coef, n_steps, batch_size, learning_rate)\n",
    "\n",
    "        print(f\"\\n{YELLOW}=== Training segment {check+1}/{n_checks} ===\")\n",
    "        print(f\"Steps: {check*CHECKPOINT_INTERVAL}-{(check+1)*CHECKPOINT_INTERVAL}\")\n",
    "        print(f\"Params: n_steps={n_steps}, batch={batch_size}, ent_coef={ent_coef:.4f}\")\n",
    "        print(f\"Opponents: {opponent_pool}{RESET}\\n\")\n",
    "\n",
    "        # Train the model\n",
    "        model.learn(total_timesteps=CHECKPOINT_INTERVAL)\n",
    "\n",
    "        # Evaluate against opponents\n",
    "        results = evaluate_model_by_opponent(model, opponent_pool, n_episodes=1000)\n",
    "        current_stats = {k: {\"defeat_rate\": v[\"defeat_rate\"], \"victory_rate\": v[\"victory_rate\"]} for k, v in results.items()}\n",
    "\n",
    "        # Save if improvement\n",
    "        if should_save_model(current_stats, best_stats, IMPROVEMENT_THRESHOLD):\n",
    "            print(f\"{GREEN}Saved new best model at checkpoint {check}{RESET}\")\n",
    "            print(f\"{RED}Old best stats -> {best_stats}{RESET}\")\n",
    "            print(f\"{YELLOW}New best stats -> {current_stats}{RESET}\")\n",
    "\n",
    "            improvement = True\n",
    "            best_stats = deepcopy(current_stats)\n",
    "            model.save(model_path)\n",
    "            save_opponent_stats(best_stats, STATS_PATH)\n",
    "\n",
    "        checkpoint_path = os.path.join(MODELS_DIR, \"last_checkpoint.zip\")\n",
    "        model.save(checkpoint_path)\n",
    "\n",
    "    return improvement\n"
   ],
   "id": "345f71fefcf28b3f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T08:31:57.761070Z",
     "start_time": "2025-08-14T08:31:57.754107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main_training_loop(nb_models_to_train=2):\n",
    "    trained_count = 0\n",
    "    while trained_count < nb_models_to_train:\n",
    "        improvement = train_one_model()\n",
    "        if improvement:\n",
    "            print(f\"{GREEN}Training completed for model {trained_count+1}. Best model saved.{RESET}\")\n",
    "            trained_count += 1\n",
    "        else:\n",
    "            print(f\"{RED}Warning: No model met improvement criteria{RESET}\")\n",
    "            # Decide if you want to break or continue anyway\n",
    "            trained_count += 1  # or break?"
   ],
   "id": "c65dd6510ed9b181",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-08-14T08:31:57.811141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run the training loop\n",
    "main_training_loop(1)"
   ],
   "id": "361448dc0742ab2e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "\n",
      "\u001B[33m=== Training segment 1/10 ===\n",
      "Steps: 0-10000\n",
      "Params: n_steps=2048, batch=512, ent_coef=0.0200\n",
      "Opponents: ['random']\u001B[0m\n",
      "\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.78     |\n",
      "|    ep_rew_mean     | -0.186   |\n",
      "| time/              |          |\n",
      "|    fps             | 534      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3.87         |\n",
      "|    ep_rew_mean          | -0.048       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 499          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044262195 |\n",
      "|    clip_fraction        | 0.00386      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.6         |\n",
      "|    explained_variance   | -0.0201      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.278        |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00724     |\n",
      "|    value_loss           | 0.687        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.7         |\n",
      "|    ep_rew_mean          | -0.194      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 497         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010801427 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.0652      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.268       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    value_loss           | 0.687       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.73        |\n",
      "|    ep_rew_mean          | -0.188      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 498         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009060135 |\n",
      "|    clip_fraction        | 0.0936      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.25        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    value_loss           | 0.623       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3.57         |\n",
      "|    ep_rew_mean          | -0.136       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 500          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057385396 |\n",
      "|    clip_fraction        | 0.0591       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.52        |\n",
      "|    explained_variance   | 0.134        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.243        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00867     |\n",
      "|    value_loss           | 0.618        |\n",
      "------------------------------------------\n",
      "\u001B[34m**********[1.0]*******\u001B[0m\n",
      "\u001B[34m**********[1.0]*******\u001B[0m\n",
      "Opponent: random\n",
      "Defeat rate: 22.70%\n",
      "Losses (play first): 54\n",
      "Losses (play second): 173\n",
      "\u001B[32mSaved new best model at checkpoint 0\u001B[0m\n",
      "\u001B[31mOld best stats -> {'random': {'defeat_rate': 1.0, 'victory_rate': 0.0}}\u001B[0m\n",
      "\u001B[33mNew best stats -> {'random': {'defeat_rate': 0.227, 'victory_rate': 0.719}}\u001B[0m\n",
      "\n",
      "\u001B[33m=== Training segment 2/10 ===\n",
      "Steps: 10000-20000\n",
      "Params: n_steps=2372, batch=665, ent_coef=0.0200\n",
      "Opponents: ['random']\u001B[0m\n",
      "\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.66     |\n",
      "|    ep_rew_mean     | -0.01    |\n",
      "| time/              |          |\n",
      "|    fps             | 535      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "\u001B[34m**********[1.0]*******\u001B[0m\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.6         |\n",
      "|    ep_rew_mean          | -0.124      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 452         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008752584 |\n",
      "|    clip_fraction        | 0.0799      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.168       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.239       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    value_loss           | 0.591       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3.73         |\n",
      "|    ep_rew_mean          | 0.062        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 415          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059639313 |\n",
      "|    clip_fraction        | 0.0781       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.214        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.195        |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0117      |\n",
      "|    value_loss           | 0.553        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3.54         |\n",
      "|    ep_rew_mean          | 0.226        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 402          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069401413 |\n",
      "|    clip_fraction        | 0.0738       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.33        |\n",
      "|    explained_variance   | 0.228        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.222        |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.014       |\n",
      "|    value_loss           | 0.547        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.58        |\n",
      "|    ep_rew_mean          | 0.142       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 392         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008698575 |\n",
      "|    clip_fraction        | 0.0838      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.261       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.208       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    value_loss           | 0.514       |\n",
      "-----------------------------------------\n",
      "\u001B[34m**********[1.0]*******\u001B[0m\n",
      "Opponent: random\n",
      "Defeat rate: 18.70%\n",
      "Losses (play first): 41\n",
      "Losses (play second): 146\n",
      "\u001B[32mSaved new best model at checkpoint 1\u001B[0m\n",
      "\u001B[31mOld best stats -> {'random': {'defeat_rate': 0.227, 'victory_rate': 0.719}}\u001B[0m\n",
      "\u001B[33mNew best stats -> {'random': {'defeat_rate': 0.187, 'victory_rate': 0.77}}\u001B[0m\n",
      "\n",
      "\u001B[33m=== Training segment 3/10 ===\n",
      "Steps: 20000-30000\n",
      "Params: n_steps=2613, batch=819, ent_coef=0.0200\n",
      "Opponents: ['random']\u001B[0m\n",
      "\n",
      "\u001B[34m**********[1.0]*******\u001B[0m\n",
      "\u001B[34m**********[1.0]*******\u001B[0m\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.48     |\n",
      "|    ep_rew_mean     | 0.178    |\n",
      "| time/              |          |\n",
      "|    fps             | 517      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3.51         |\n",
      "|    ep_rew_mean          | 0.094        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 502          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065851584 |\n",
      "|    clip_fraction        | 0.099        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.13        |\n",
      "|    explained_variance   | 0.336        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.16         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.0173      |\n",
      "|    value_loss           | 0.443        |\n",
      "------------------------------------------\n",
      "\u001B[34m**********[1.0]*******\u001B[0m\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3.44         |\n",
      "|    ep_rew_mean          | 0.35         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 487          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068062423 |\n",
      "|    clip_fraction        | 0.0536       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.13        |\n",
      "|    explained_variance   | 0.236        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.168        |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.0163      |\n",
      "|    value_loss           | 0.483        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.31        |\n",
      "|    ep_rew_mean          | 0.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 491         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007880407 |\n",
      "|    clip_fraction        | 0.0609      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.307       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.183       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0163     |\n",
      "|    value_loss           | 0.47        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3.37         |\n",
      "|    ep_rew_mean          | 0.41         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 469          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061937342 |\n",
      "|    clip_fraction        | 0.0526       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.359        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.155        |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.0165      |\n",
      "|    value_loss           | 0.41         |\n",
      "------------------------------------------\n",
      "Opponent: random\n",
      "Defeat rate: 11.20%\n",
      "Losses (play first): 16\n",
      "Losses (play second): 96\n",
      "\u001B[32mSaved new best model at checkpoint 2\u001B[0m\n",
      "\u001B[31mOld best stats -> {'random': {'defeat_rate': 0.187, 'victory_rate': 0.77}}\u001B[0m\n",
      "\u001B[33mNew best stats -> {'random': {'defeat_rate': 0.112, 'victory_rate': 0.86}}\u001B[0m\n",
      "\n",
      "\u001B[33m=== Training segment 4/10 ===\n",
      "Steps: 30000-40000\n",
      "Params: n_steps=2829, batch=972, ent_coef=0.0200\n",
      "Opponents: ['random']\u001B[0m\n",
      "\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.45     |\n",
      "|    ep_rew_mean     | 0.464    |\n",
      "| time/              |          |\n",
      "|    fps             | 486      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

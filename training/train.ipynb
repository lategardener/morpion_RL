{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T23:26:03.958430Z",
     "start_time": "2025-08-14T23:26:01.836478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from envs import TicTacToeTrainingEnv\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "from utils.terminal_colors import *\n",
    "from utils.json_utils import save_opponent_stats, load_opponent_stats\n",
    "from utils.models_utils import should_save_model, get_models, get_last_model_number\n",
    "from utils.evaluator import evaluate_model_by_opponent\n",
    "from training.config import *\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from test.action_mask_ import mask_fn"
   ],
   "id": "303d68e3ed027acc",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1 style=\"color:#0b9ed8\">TRAINING</h1>",
   "id": "54afbc8fa5be6463"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T23:26:03.978086Z",
     "start_time": "2025-08-14T23:26:03.966392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_env(opponent_pool):\n",
    "    \"\"\"Create and wrap the training environment once per training session.\"\"\"\n",
    "    env_init = TicTacToeTrainingEnv(\n",
    "        board_length=TRAINING_DEFAULT_BOARD_LENGTH,\n",
    "        pattern_victory_length=TRAINING_DEFAULT_PATTERN_VICTORY_LENGTH,\n",
    "        opponent_pool=opponent_pool,\n",
    "        first_play_rate=0.4,\n",
    "        lost_games_path=DEFEAT_PATH,\n",
    "        review_ratio=0.2,\n",
    "        opponent_statistics_file=STATS_PATH,\n",
    "    )\n",
    "    env = ActionMasker(env_init, mask_fn)\n",
    "    env.reset()\n",
    "    return env"
   ],
   "id": "1cd234e3712da8db",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T23:26:04.134492Z",
     "start_time": "2025-08-14T23:26:04.129754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_model(env, last_model_num, ent_coef, n_steps, batch_size, learning_rate):\n",
    "    checkpoint_path = os.path.join(MODELS_DIR, \"last_checkpoint.zip\")\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        model = MaskablePPO.load(checkpoint_path, env=env)\n",
    "        print(\"✅ Loaded model from last checkpoint.\")\n",
    "    elif last_model_num == 0:\n",
    "        model = MaskablePPO(\n",
    "            \"MultiInputPolicy\",\n",
    "            env=env,\n",
    "            verbose=1,\n",
    "            gamma=GAMMA,\n",
    "            gae_lambda=GAE_LAMBDA,\n",
    "            ent_coef=ent_coef,\n",
    "            n_steps=n_steps,\n",
    "            batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            policy_kwargs=policy_kwargs\n",
    "        )\n",
    "    else:\n",
    "        prev_model_path = get_models(MODELS_DIR)[-1]\n",
    "        model = MaskablePPO.load(prev_model_path, env=env)\n",
    "\n",
    "    # Mettre à jour les paramètres dynamiques\n",
    "    model.ent_coef = ent_coef\n",
    "    model.n_steps = n_steps\n",
    "    model.batch_size = batch_size\n",
    "    model.learning_rate = learning_rate\n",
    "    return model\n"
   ],
   "id": "505a200d48e06172",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T23:26:04.182634Z",
     "start_time": "2025-08-14T23:26:04.174270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_one_model():\n",
    "    last_model_num = get_last_model_number(MODELS_DIR)\n",
    "    next_model_num = last_model_num + 1\n",
    "    model_name = f\"{BASE_MODELS_NAME}_{next_model_num}.zip\"\n",
    "    model_path = os.path.join(MODELS_DIR, model_name)\n",
    "\n",
    "    opponent_models = get_models(MODELS_DIR)\n",
    "    opponent_pool = [\"random\", \"smart_random\"] + opponent_models\n",
    "    improvement = False\n",
    "    best_stats = load_opponent_stats(opponent_pool)\n",
    "    n_checks = TOTAL_STEPS // CHECKPOINT_INTERVAL\n",
    "\n",
    "    # Create environment once\n",
    "    env = create_env(opponent_pool)\n",
    "\n",
    "    for check in range(n_checks):\n",
    "        current_progress = (check * CHECKPOINT_INTERVAL) / TOTAL_STEPS\n",
    "\n",
    "        # Dynamic training parameters\n",
    "        n_steps = int(2048 + (4096 - 2048) * current_progress**0.8)\n",
    "        batch_size = min(1024, int(512 + (2048 - 512) * current_progress**1.0))\n",
    "        ent_coef = 0.01\n",
    "        learning_rate = LR_SCHEDULE(current_progress)\n",
    "\n",
    "        # Initialize or load model only on first checkpoint\n",
    "        if check == 0:\n",
    "            model = initialize_model(env, last_model_num, ent_coef, n_steps, batch_size, learning_rate)\n",
    "\n",
    "        print(f\"\\n{YELLOW}=== Training segment {check+1}/{n_checks} ===\")\n",
    "        print(f\"Steps: {check*CHECKPOINT_INTERVAL}-{(check+1)*CHECKPOINT_INTERVAL}\")\n",
    "        print(f\"Params: n_steps={n_steps}, batch={batch_size}, ent_coef={ent_coef:.4f}\")\n",
    "        print(f\"Opponents: {opponent_pool}{RESET}\\n\")\n",
    "\n",
    "        # Train the model\n",
    "        model.learn(total_timesteps=CHECKPOINT_INTERVAL)\n",
    "\n",
    "        # Evaluate against opponents\n",
    "        results = evaluate_model_by_opponent(model, opponent_pool, n_episodes=2000)\n",
    "        current_stats = {k: {\"defeat_rate\": v[\"defeat_rate\"], \"victory_rate\": v[\"victory_rate\"]} for k, v in results.items()}\n",
    "\n",
    "        # Save if improvement\n",
    "        if should_save_model(current_stats, best_stats, IMPROVEMENT_THRESHOLD):\n",
    "            print(f\"{GREEN}Saved new best model at checkpoint {check}{RESET}\")\n",
    "            print(f\"{RED}Old best stats -> {best_stats}{RESET}\")\n",
    "            print(f\"{YELLOW}New best stats -> {current_stats}{RESET}\")\n",
    "\n",
    "            improvement = True\n",
    "            best_stats = deepcopy(current_stats)\n",
    "            model.save(model_path)\n",
    "            save_opponent_stats(best_stats, STATS_PATH)\n",
    "\n",
    "        checkpoint_path = os.path.join(MODELS_DIR, \"last_checkpoint.zip\")\n",
    "        model.save(checkpoint_path)\n",
    "\n",
    "    return improvement\n"
   ],
   "id": "345f71fefcf28b3f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T23:26:04.237358Z",
     "start_time": "2025-08-14T23:26:04.230818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main_training_loop(nb_models_to_train=2):\n",
    "    trained_count = 0\n",
    "    while trained_count < nb_models_to_train:\n",
    "        improvement = train_one_model()\n",
    "        if improvement:\n",
    "            print(f\"{GREEN}Training completed for model {trained_count+1}. Best model saved.{RESET}\")\n",
    "            trained_count += 1\n",
    "        else:\n",
    "            print(f\"{RED}Warning: No model met improvement criteria{RESET}\")\n",
    "            # Decide if you want to break or continue anyway\n",
    "            trained_count += 1  # or break?"
   ],
   "id": "c65dd6510ed9b181",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T23:32:56.713394Z",
     "start_time": "2025-08-14T23:26:04.287616Z"
    }
   },
   "cell_type": "code",
   "source": [
    " # Run the training loop\n",
    "main_training_loop(1)"
   ],
   "id": "361448dc0742ab2e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "\n",
      "\u001B[33m=== Training segment 1/10 ===\n",
      "Steps: 0-10000\n",
      "Params: n_steps=2048, batch=512, ent_coef=0.0100\n",
      "Opponents: ['random', 'smart_random']\u001B[0m\n",
      "\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 8.44     |\n",
      "|    ep_rew_mean     | -0.495   |\n",
      "| time/              |          |\n",
      "|    fps             | 87       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.3          |\n",
      "|    ep_rew_mean          | -0.389       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 89           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 45           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050284206 |\n",
      "|    clip_fraction        | 0.00254      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.66        |\n",
      "|    explained_variance   | -0.0462      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.23         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00945     |\n",
      "|    value_loss           | 0.626        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.45        |\n",
      "|    ep_rew_mean          | -0.049      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 69          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011486429 |\n",
      "|    clip_fraction        | 0.0812      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.66       |\n",
      "|    explained_variance   | 0.086       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.196       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    value_loss           | 0.565       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.41         |\n",
      "|    ep_rew_mean          | -0.0858      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 90           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 90           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0110313315 |\n",
      "|    clip_fraction        | 0.0468       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.65        |\n",
      "|    explained_variance   | -0.0749      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.21         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00922     |\n",
      "|    value_loss           | 0.63         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.46         |\n",
      "|    ep_rew_mean          | 0.158        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 91           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 112          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0117879305 |\n",
      "|    clip_fraction        | 0.0944       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.62        |\n",
      "|    explained_variance   | -0.0114      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.219        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0132      |\n",
      "|    value_loss           | 0.652        |\n",
      "------------------------------------------\n",
      "Opponent: random\n",
      "Defeat rate: 20.30%\n",
      "Losses (play first): 156\n",
      "Losses (play second): 250\n",
      "Opponent: smart_random\n",
      "Defeat rate: 73.65%\n",
      "Losses (play first): 668\n",
      "Losses (play second): 805\n",
      "\u001B[32mSaved new best model at checkpoint 0\u001B[0m\n",
      "\u001B[31mOld best stats -> {'random': {'defeat_rate': 1.0, 'victory_rate': 0.0}, 'smart_random': {'defeat_rate': 1.0, 'victory_rate': 0.0}}\u001B[0m\n",
      "\u001B[33mNew best stats -> {'random': {'defeat_rate': 0.203, 'victory_rate': 0.7945}, 'smart_random': {'defeat_rate': 0.7365, 'victory_rate': 0.255}}\u001B[0m\n",
      "\n",
      "\u001B[33m=== Training segment 2/10 ===\n",
      "Steps: 10000-20000\n",
      "Params: n_steps=2372, batch=665, ent_coef=0.0100\n",
      "Opponents: ['random', 'smart_random']\u001B[0m\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Run the training loop\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mmain_training_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[5], line 4\u001B[0m, in \u001B[0;36mmain_training_loop\u001B[0;34m(nb_models_to_train)\u001B[0m\n\u001B[1;32m      2\u001B[0m trained_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m trained_count \u001B[38;5;241m<\u001B[39m nb_models_to_train:\n\u001B[0;32m----> 4\u001B[0m     improvement \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_one_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m improvement:\n\u001B[1;32m      6\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mGREEN\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mTraining completed for model \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrained_count\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Best model saved.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mRESET\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[4], line 35\u001B[0m, in \u001B[0;36mtrain_one_model\u001B[0;34m()\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOpponents: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mopponent_pool\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mRESET\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[0;32m---> 35\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mCHECKPOINT_INTERVAL\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# Evaluate against opponents\u001B[39;00m\n\u001B[1;32m     38\u001B[0m results \u001B[38;5;241m=\u001B[39m evaluate_model_by_opponent(model, opponent_pool, n_episodes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2000\u001B[39m)\n",
      "File \u001B[0;32m~/PROGRAMMATION/PYTHON/PERSO/EXERCISM/venv/lib/python3.10/site-packages/sb3_contrib/ppo_mask/ppo_mask.py:454\u001B[0m, in \u001B[0;36mMaskablePPO.learn\u001B[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, use_masking, progress_bar)\u001B[0m\n\u001B[1;32m    451\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    453\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps \u001B[38;5;241m<\u001B[39m total_timesteps:\n\u001B[0;32m--> 454\u001B[0m     continue_training \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect_rollouts\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrollout_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_steps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_masking\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    456\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m continue_training:\n\u001B[1;32m    457\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/PROGRAMMATION/PYTHON/PERSO/EXERCISM/venv/lib/python3.10/site-packages/sb3_contrib/ppo_mask/ppo_mask.py:233\u001B[0m, in \u001B[0;36mMaskablePPO.collect_rollouts\u001B[0;34m(self, env, callback, rollout_buffer, n_rollout_steps, use_masking)\u001B[0m\n\u001B[1;32m    230\u001B[0m     actions, values, log_probs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy(obs_tensor, action_masks\u001B[38;5;241m=\u001B[39maction_masks)\n\u001B[1;32m    232\u001B[0m actions \u001B[38;5;241m=\u001B[39m actions\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m--> 233\u001B[0m new_obs, rewards, dones, infos \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mactions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mnum_envs\n\u001B[1;32m    237\u001B[0m \u001B[38;5;66;03m# Give access to local variables\u001B[39;00m\n",
      "File \u001B[0;32m~/PROGRAMMATION/PYTHON/PERSO/EXERCISM/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:222\u001B[0m, in \u001B[0;36mVecEnv.step\u001B[0;34m(self, actions)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    216\u001B[0m \u001B[38;5;124;03mStep the environments with the given action\u001B[39;00m\n\u001B[1;32m    217\u001B[0m \n\u001B[1;32m    218\u001B[0m \u001B[38;5;124;03m:param actions: the action\u001B[39;00m\n\u001B[1;32m    219\u001B[0m \u001B[38;5;124;03m:return: observation, reward, done, information\u001B[39;00m\n\u001B[1;32m    220\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    221\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep_async(actions)\n\u001B[0;32m--> 222\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep_wait\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PROGRAMMATION/PYTHON/PERSO/EXERCISM/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:59\u001B[0m, in \u001B[0;36mDummyVecEnv.step_wait\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mstep_wait\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m VecEnvStepReturn:\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;66;03m# Avoid circular imports\u001B[39;00m\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m env_idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_envs):\n\u001B[0;32m---> 59\u001B[0m         obs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_rews[env_idx], terminated, truncated, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_infos[env_idx] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menvs\u001B[49m\u001B[43m[\u001B[49m\u001B[43menv_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[assignment]\u001B[39;49;00m\n\u001B[1;32m     60\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mactions\u001B[49m\u001B[43m[\u001B[49m\u001B[43menv_idx\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     61\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     62\u001B[0m         \u001B[38;5;66;03m# convert to SB3 VecEnv api\u001B[39;00m\n\u001B[1;32m     63\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_dones[env_idx] \u001B[38;5;241m=\u001B[39m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated\n",
      "File \u001B[0;32m~/PROGRAMMATION/PYTHON/PERSO/EXERCISM/venv/lib/python3.10/site-packages/stable_baselines3/common/monitor.py:94\u001B[0m, in \u001B[0;36mMonitor.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mneeds_reset:\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTried to step environment that needs reset\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 94\u001B[0m observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrewards\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mfloat\u001B[39m(reward))\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m terminated \u001B[38;5;129;01mor\u001B[39;00m truncated:\n",
      "File \u001B[0;32m~/PROGRAMMATION/PYTHON/PERSO/EXERCISM/venv/lib/python3.10/site-packages/gymnasium/core.py:327\u001B[0m, in \u001B[0;36mWrapper.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    323\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mstep\u001B[39m(\n\u001B[1;32m    324\u001B[0m     \u001B[38;5;28mself\u001B[39m, action: WrapperActType\n\u001B[1;32m    325\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[WrapperObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[1;32m    326\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 327\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PROGRAMMATION/AI/tic_tac_toe_rl/envs/training_env.py:273\u001B[0m, in \u001B[0;36mTicTacToeTrainingEnv.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    270\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluation:\n\u001B[1;32m    271\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mopponent_blows\u001B[38;5;241m.\u001B[39mappend(opponent_action)\n\u001B[0;32m--> 273\u001B[0m obs_opponent, reward_opponent, done_opponent, truncated_opponent, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mopponent_action\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    275\u001B[0m \u001B[38;5;66;03m# Check for opponent win or draw\u001B[39;00m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m done_opponent:\n",
      "File \u001B[0;32m~/PROGRAMMATION/AI/tic_tac_toe_rl/envs/base_env.py:159\u001B[0m, in \u001B[0;36mTicTacToeBaseEnv.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    157\u001B[0m         reward \u001B[38;5;241m=\u001B[39m REWARD_MISSED_WIN\n\u001B[1;32m    158\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 159\u001B[0m         reward \u001B[38;5;241m=\u001B[39m \u001B[43mheuristic_points\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplayer\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplayer\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgameboard\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mboard_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    162\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpattern_victory_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalid_actions\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    163\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[38;5;66;03m# Switch to the other player\u001B[39;00m\n\u001B[1;32m    166\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mplayer \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mplayer\n",
      "File \u001B[0;32m~/PROGRAMMATION/AI/tic_tac_toe_rl/utils/heuristics.py:803\u001B[0m, in \u001B[0;36mheuristic_points\u001B[0;34m(playerId, opponentId, board, size, length_victory_pattern, authorized_moves)\u001B[0m\n\u001B[1;32m    800\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_winning_move(opponentId, board, size, length_victory_pattern, authorized_moves) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    801\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m REWARD_ALLOW_OPP_WIN\n\u001B[0;32m--> 803\u001B[0m reward \u001B[38;5;241m=\u001B[39m agent_heuristic_points_calcul(playerId, opponentId, board, size, length_victory_pattern) \u001B[38;5;241m-\u001B[39m \u001B[43magent_heuristic_points_calcul\u001B[49m\u001B[43m(\u001B[49m\u001B[43mopponentId\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mplayerId\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mboard\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlength_victory_pattern\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    804\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m reward\n",
      "File \u001B[0;32m~/PROGRAMMATION/AI/tic_tac_toe_rl/utils/heuristics.py:786\u001B[0m, in \u001B[0;36magent_heuristic_points_calcul\u001B[0;34m(playerId, opponentId, board, size, length_victory_pattern)\u001B[0m\n\u001B[1;32m    773\u001B[0m     score \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    774\u001B[0m             \u001B[38;5;241m0.05\u001B[39m \u001B[38;5;241m*\u001B[39m number_of_semi_opened_threats(length_victory_pattern \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m2\u001B[39m, playerId, opponentId, board, size, length_victory_pattern) \u001B[38;5;241m+\u001B[39m\n\u001B[1;32m    775\u001B[0m             \u001B[38;5;66;03m# 0.01 * number_of_dangerous_semi_opened_threats(length_victory_pattern - 2, playerId, opponentId, board, size, length_victory_pattern) +\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    779\u001B[0m             \u001B[38;5;241m0.2\u001B[39m \u001B[38;5;241m*\u001B[39m number_of_opened_threats(length_victory_pattern \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m, playerId, board, size)\n\u001B[1;32m    780\u001B[0m     )\n\u001B[1;32m    781\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    782\u001B[0m     score \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    783\u001B[0m             \u001B[38;5;241m0.05\u001B[39m \u001B[38;5;241m*\u001B[39m number_of_semi_opened_threats(length_victory_pattern \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m2\u001B[39m, playerId, opponentId, board, size, length_victory_pattern) \u001B[38;5;241m+\u001B[39m\n\u001B[1;32m    784\u001B[0m             \u001B[38;5;241m0.06\u001B[39m \u001B[38;5;241m*\u001B[39m number_of_dangerous_semi_opened_threats(length_victory_pattern \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m2\u001B[39m, playerId, opponentId, board, size, length_victory_pattern) \u001B[38;5;241m+\u001B[39m\n\u001B[1;32m    785\u001B[0m             \u001B[38;5;241m0.05\u001B[39m \u001B[38;5;241m*\u001B[39m number_of_opened_threats(length_victory_pattern \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m2\u001B[39m, playerId, board, size) \u001B[38;5;241m+\u001B[39m\n\u001B[0;32m--> 786\u001B[0m             \u001B[38;5;241m0.075\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[43mnumber_of_semi_opened_threats\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlength_victory_pattern\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mplayerId\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopponentId\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mboard\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlength_victory_pattern\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m\n\u001B[1;32m    787\u001B[0m             \u001B[38;5;241m0.09\u001B[39m \u001B[38;5;241m*\u001B[39m number_of_dangerous_semi_opened_threats(length_victory_pattern \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m, playerId, opponentId, board, size, length_victory_pattern) \u001B[38;5;241m+\u001B[39m\n\u001B[1;32m    788\u001B[0m             \u001B[38;5;241m0.15\u001B[39m \u001B[38;5;241m*\u001B[39m number_of_opened_threats(length_victory_pattern \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m, playerId, board, size)\n\u001B[1;32m    789\u001B[0m     )\n\u001B[1;32m    790\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m score\n",
      "File \u001B[0;32m~/PROGRAMMATION/AI/tic_tac_toe_rl/utils/heuristics.py:675\u001B[0m, in \u001B[0;36mnumber_of_semi_opened_threats\u001B[0;34m(threat_length, playerId, opponentId, board, size, pattern_victory_length)\u001B[0m\n\u001B[1;32m    667\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mnumber_of_semi_opened_threats\u001B[39m(threat_length, playerId, opponentId, board, size, pattern_victory_length):\n\u001B[1;32m    668\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    669\u001B[0m \u001B[38;5;124;03m    Calculate total number of semi-opened threats for a player across the board.\u001B[39;00m\n\u001B[1;32m    670\u001B[0m \u001B[38;5;124;03m    Sums threats found on rows, columns, descending and ascending diagonals.\u001B[39;00m\n\u001B[1;32m    671\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m    672\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m    673\u001B[0m             number_of_semi_opened_threats_on_rows(threat_length, playerId, opponentId, board, pattern_victory_length) \u001B[38;5;241m+\u001B[39m\n\u001B[1;32m    674\u001B[0m             number_of_semi_opened_threats_on_columns(threat_length, playerId, opponentId, board, size, pattern_victory_length) \u001B[38;5;241m+\u001B[39m\n\u001B[0;32m--> 675\u001B[0m             \u001B[43mnumber_of_semi_opened_threats_on_descending_diagonals\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthreat_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mplayerId\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopponentId\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mboard\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpattern_victory_length\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m\n\u001B[1;32m    676\u001B[0m             number_of_semi_opened_threats_on_ascending_diagonals(threat_length, playerId, opponentId, board, size, pattern_victory_length)\n\u001B[1;32m    677\u001B[0m     )\n",
      "File \u001B[0;32m~/PROGRAMMATION/AI/tic_tac_toe_rl/utils/heuristics.py:544\u001B[0m, in \u001B[0;36mnumber_of_semi_opened_threats_on_descending_diagonals\u001B[0;34m(threat_length, playerId, opponentId, board, size, pattern_victory_length)\u001B[0m\n\u001B[1;32m    542\u001B[0m         i \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    543\u001B[0m         j \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 544\u001B[0m     count \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mcontains_all_semi_opened_threats\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdiagonal\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mthreat_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mplayerId\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopponentId\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpattern_victory_length\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    546\u001B[0m \u001B[38;5;66;03m# Descending diagonals starting from first column (excluding the main diagonal already counted)\u001B[39;00m\n\u001B[1;32m    547\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m y \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, size):\n",
      "File \u001B[0;32m~/PROGRAMMATION/AI/tic_tac_toe_rl/utils/heuristics.py:394\u001B[0m, in \u001B[0;36mcontains_all_semi_opened_threats\u001B[0;34m(segment, threat_length, playerId, opponentId, pattern_victory_length)\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    381\u001B[0m \u001B[38;5;124;03mCounts how many semi-opened threat patterns are contained in a given board segment (string).\u001B[39;00m\n\u001B[1;32m    382\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    391\u001B[0m \u001B[38;5;124;03m    int: Total count of detected semi-opened threat patterns in the segment.\u001B[39;00m\n\u001B[1;32m    392\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    393\u001B[0m \u001B[38;5;66;03m# Generate all semi-opened threat patterns and wall-blocked patterns\u001B[39;00m\n\u001B[0;32m--> 394\u001B[0m patterns, wall_blocked, _ \u001B[38;5;241m=\u001B[39m \u001B[43mpattern_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mplayerId\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mthreat_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopponentId\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpattern_victory_length\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    396\u001B[0m pattern_find_on_the_left \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    397\u001B[0m pattern_find_on_the_right \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/PROGRAMMATION/AI/tic_tac_toe_rl/utils/heuristics.py:349\u001B[0m, in \u001B[0;36mpattern_\u001B[0;34m(playerId, length, opponentId, pattern_victory_length)\u001B[0m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;66;03m# Generate all unique permutations of the base data\u001B[39;00m\n\u001B[1;32m    347\u001B[0m all_permutations \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(permutations(data))\n\u001B[0;32m--> 349\u001B[0m filtered_permutations \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mlist\u001B[39m(p) \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m all_permutations]\n\u001B[1;32m    351\u001B[0m \u001B[38;5;66;03m# Wall-blocked threats: patterns starting and ending with playerId (fully blocked on both ends)\u001B[39;00m\n\u001B[1;32m    352\u001B[0m wall_blocked \u001B[38;5;241m=\u001B[39m [threat \u001B[38;5;28;01mfor\u001B[39;00m threat \u001B[38;5;129;01min\u001B[39;00m filtered_permutations \u001B[38;5;28;01mif\u001B[39;00m threat[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m playerId \u001B[38;5;129;01mand\u001B[39;00m threat[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m playerId]\n",
      "File \u001B[0;32m~/PROGRAMMATION/AI/tic_tac_toe_rl/utils/heuristics.py:349\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;66;03m# Generate all unique permutations of the base data\u001B[39;00m\n\u001B[1;32m    347\u001B[0m all_permutations \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(permutations(data))\n\u001B[0;32m--> 349\u001B[0m filtered_permutations \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mlist\u001B[39m(p) \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m all_permutations]\n\u001B[1;32m    351\u001B[0m \u001B[38;5;66;03m# Wall-blocked threats: patterns starting and ending with playerId (fully blocked on both ends)\u001B[39;00m\n\u001B[1;32m    352\u001B[0m wall_blocked \u001B[38;5;241m=\u001B[39m [threat \u001B[38;5;28;01mfor\u001B[39;00m threat \u001B[38;5;129;01min\u001B[39;00m filtered_permutations \u001B[38;5;28;01mif\u001B[39;00m threat[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m playerId \u001B[38;5;129;01mand\u001B[39;00m threat[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m playerId]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

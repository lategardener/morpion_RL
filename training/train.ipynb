{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T12:02:31.640333Z",
     "start_time": "2025-08-20T12:02:27.986559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from envs import TicTacToeTrainingEnv\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from copy import deepcopy\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "from utils.terminal_colors import *\n",
    "from utils.json_utils import save_opponent_stats, load_opponent_stats\n",
    "from utils.agents_utils import should_save_agent, get_agents, get_last_agent_number\n",
    "from utils.evaluator import evaluate_agent_by_opponent\n",
    "from utils.visualize import defeat_rate_plot\n",
    "from training.config import *\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from utils.action_mask_ import mask_fn\n",
    "import json"
   ],
   "id": "48163c52afbd6247",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h1 style=\"color:#0b9ed8\">TRAINING</h1>",
   "id": "54afbc8fa5be6463"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T12:02:31.656656Z",
     "start_time": "2025-08-20T12:02:31.648903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_env(opponent_pool):\n",
    "    \"\"\"\n",
    "    Create and wrap the TicTacToe training environment once per training session.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    opponent_pool : list of opponent names or agents against which the agent will train.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    env : ActionMasker\n",
    "        Wrapped TicTacToe environment ready for training.\n",
    "    \"\"\"\n",
    "    env_init = TicTacToeTrainingEnv(\n",
    "        board_length=TRAINING_DEFAULT_BOARD_LENGTH,\n",
    "        pattern_victory_length=TRAINING_DEFAULT_PATTERN_VICTORY_LENGTH,\n",
    "        opponent_pool=opponent_pool,\n",
    "        first_play_rate=TRAINING_DEFAULT_FIRST_PLAY_RATE,\n",
    "        lost_games_path=DEFEAT_PATH,\n",
    "        review_ratio=TRAINING_DEFAULT_REVIEW_RATIO,\n",
    "        opponent_statistics_file=BEST_STATS_PATH,\n",
    "    )\n",
    "    env = ActionMasker(env_init, mask_fn)\n",
    "    env.reset()\n",
    "    return env"
   ],
   "id": "1cd234e3712da8db",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T12:02:31.824478Z",
     "start_time": "2025-08-20T12:02:31.818736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_agent(env, last_agent_num, ent_coef, n_steps, batch_size, learning_rate):\n",
    "    \"\"\"\n",
    "    Initialize or load the PPO agent. Updates dynamic training parameters if agent exists.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    env : ActionMasker\n",
    "        The training environment.\n",
    "    last_agent_num : int\n",
    "        The number of the last saved agent.\n",
    "    ent_coef : float\n",
    "        Entropy coefficient for exploration.\n",
    "    n_steps : int\n",
    "        Number of steps to run for each environment per update.\n",
    "    batch_size : int\n",
    "        Size of minibatches for training.\n",
    "    learning_rate : float\n",
    "        Learning rate for training.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    agent : MaskablePPO\n",
    "        Initialized or loaded PPO agent.\n",
    "    \"\"\"\n",
    "    checkpoint_path = os.path.join(AGENTS_DIR, \"last_checkpoint.zip\")\n",
    "\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        agent = MaskablePPO.load(checkpoint_path, env=env)\n",
    "        print(\"âœ… Loaded agent from last checkpoint.\")\n",
    "    elif last_agent_num == 0:\n",
    "        agent = MaskablePPO(\n",
    "            \"MultiInputPolicy\",\n",
    "            env=env,\n",
    "            verbose=1,\n",
    "            gamma=GAMMA,\n",
    "            gae_lambda=GAE_LAMBDA,\n",
    "            ent_coef=ent_coef,\n",
    "            n_steps=n_steps,\n",
    "            batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            policy_kwargs=policy_kwargs\n",
    "        )\n",
    "    else:\n",
    "        prev_agent_path = get_agents(AGENTS_DIR)[-1]\n",
    "        agent = MaskablePPO.load(prev_agent_path, env=env)\n",
    "\n",
    "    # Update dynamic parameters\n",
    "    agent.ent_coef = ent_coef\n",
    "    agent.n_steps = n_steps\n",
    "    agent.batch_size = batch_size\n",
    "    agent.learning_rate = learning_rate\n",
    "    return agent"
   ],
   "id": "7b2dafd047651823",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T12:02:31.884386Z",
     "start_time": "2025-08-20T12:02:31.865795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_one_agent():\n",
    "    \"\"\"\n",
    "    Train a single PPO agent, evaluate against opponents, and save stats continuously.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    improvement : bool\n",
    "        True if agent shows improvement over previous best.\n",
    "    \"\"\"\n",
    "    last_agent_num = get_last_agent_number(AGENTS_DIR)\n",
    "    next_agent_num = last_agent_num + 1\n",
    "    agent_name = f\"agent_v{next_agent_num}_{BASE_AGENTS_NAME}.zip\"\n",
    "    agent_path = os.path.join(AGENTS_DIR, agent_name)\n",
    "\n",
    "    opponent_agents = get_agents(AGENTS_DIR)\n",
    "    opponent_pool = [\"random\", \"smart_random\"] + opponent_agents\n",
    "    improvement = False\n",
    "    best_stats = load_opponent_stats(opponent_pool)\n",
    "    n_checks = TOTAL_STEPS // CHECKPOINT_INTERVAL\n",
    "\n",
    "    env = create_env(opponent_pool)\n",
    "\n",
    "    for check in range(n_checks):\n",
    "        current_progress = (check * CHECKPOINT_INTERVAL) / TOTAL_STEPS\n",
    "\n",
    "        # Dynamic training parameters\n",
    "        n_steps = int(2048 + (4096 - 2048) * current_progress**0.8)\n",
    "        batch_size = min(1024, int(512 + (2048 - 512) * current_progress**1.0))\n",
    "        ent_coef = 0.02\n",
    "        learning_rate = LR_SCHEDULE(current_progress)\n",
    "\n",
    "        if check == 0:\n",
    "            agent = initialize_agent(env, last_agent_num, ent_coef, n_steps, batch_size, learning_rate)\n",
    "\n",
    "        print(f\"\\n{YELLOW}=== Training segment {check+1}/{n_checks} ===\")\n",
    "        print(f\"Steps: {check*CHECKPOINT_INTERVAL}-{(check+1)*CHECKPOINT_INTERVAL}\")\n",
    "        print(f\"Params: n_steps={n_steps}, batch={batch_size}, ent_coef={ent_coef:.4f}\")\n",
    "        print(f\"Opponents: {opponent_pool}{RESET}\\n\")\n",
    "\n",
    "        # Train agent\n",
    "        agent.learn(total_timesteps=CHECKPOINT_INTERVAL)\n",
    "\n",
    "        # Evaluate agent\n",
    "        results = evaluate_agent_by_opponent(agent, opponent_pool, n_episodes=2000)\n",
    "        current_stats = {k: {\"defeat_rate\": v[\"defeat_rate\"], \"victory_rate\": v[\"victory_rate\"]} for k, v in results.items()}\n",
    "\n",
    "        # Save improvement if criteria met\n",
    "        if should_save_agent(current_stats, best_stats, IMPROVEMENT_THRESHOLD):\n",
    "            print(f\"{GREEN}Saved new best agent at checkpoint {check}{RESET}\")\n",
    "            improvement = True\n",
    "            best_stats = deepcopy(current_stats)\n",
    "            agent.save(agent_path)\n",
    "            save_opponent_stats(best_stats, BEST_STATS_PATH)\n",
    "\n",
    "        # Save all stats to JSON file continuously\n",
    "        all_stats_data = {}\n",
    "        if os.path.exists(ALL_STATS_PATH):\n",
    "            with open(ALL_STATS_PATH, \"r\") as f:\n",
    "                all_stats_data = json.load(f)\n",
    "\n",
    "        # Determine the next checkpoint index based on existing entries\n",
    "        next_checkpoint = len(all_stats_data) + 1\n",
    "\n",
    "        # Store current evaluation for this checkpoint\n",
    "        all_stats_data[f\"checkpoint_{next_checkpoint}\"] = {\n",
    "            opp: {\n",
    "                \"overall_defeat_rate\": results[opp][\"defeat_rate\"],\n",
    "                \"first_player_defeat_rate\": results[opp][\"losses_play_first\"] / 1000,\n",
    "                \"second_player_defeat_rate\": results[opp][\"losses_play_second\"] / 1000,\n",
    "            }\n",
    "            for opp in opponent_pool\n",
    "        }\n",
    "\n",
    "\n",
    "        with open(ALL_STATS_PATH, \"w\") as f:\n",
    "            json.dump(all_stats_data, f, indent=4)\n",
    "\n",
    "        # Early stopping if all defeat rates are zero\n",
    "        all_defeat_zero = all(stats[\"defeat_rate\"] == 0.0 for stats in current_stats.values())\n",
    "        if all_defeat_zero:\n",
    "            print(f\"{GREEN}=== All defeat rates are 0. Early stopping triggered. ==={RESET}\")\n",
    "            break\n",
    "\n",
    "    agent.save(os.path.join(AGENTS_DIR, \"last_checkpoint.zip\"))\n",
    "    return improvement"
   ],
   "id": "691d8eaa0a5531b6",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T12:02:31.931918Z",
     "start_time": "2025-08-20T12:02:31.925619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main_training_loop(nb_agents_to_train=2):\n",
    "    \"\"\"\n",
    "    Main training loop to train multiple PPO agents sequentially.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    nb_agents_to_train : int\n",
    "        Number of agents to train in this session.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    trained_count = 0\n",
    "    while trained_count < nb_agents_to_train:\n",
    "        improvement = train_one_agent()  # now returns only improvement\n",
    "        if improvement:\n",
    "            print(f\"{GREEN}Training completed for agent {trained_count+1}. Best agent saved.{RESET}\")\n",
    "        else:\n",
    "            print(f\"{RED}Warning: No agent met improvement criteria{RESET}\")\n",
    "        trained_count += 1  # continue to next agent regardless\n"
   ],
   "id": "b5745c7fe5ea057d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T12:04:48.562982Z",
     "start_time": "2025-08-20T12:02:31.979254Z"
    }
   },
   "cell_type": "code",
   "source": [
    " # Run the training loop\n",
    "main_training_loop(1)"
   ],
   "id": "be49845952d90eff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "\n",
      "\u001B[33m=== Training segment 1/10 ===\n",
      "Steps: 0-10000\n",
      "Params: n_steps=2048, batch=512, ent_coef=0.0200\n",
      "Opponents: ['random', 'smart_random']\u001B[0m\n",
      "\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.36     |\n",
      "|    ep_rew_mean     | -0.387   |\n",
      "| time/              |          |\n",
      "|    fps             | 227      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3.46         |\n",
      "|    ep_rew_mean          | -0.279       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 223          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077431537 |\n",
      "|    clip_fraction        | 0.0123       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | -0.0206      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.317        |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00991     |\n",
      "|    value_loss           | 0.789        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.46        |\n",
      "|    ep_rew_mean          | -0.253      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 223         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012093884 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.61       |\n",
      "|    explained_variance   | 0.0617      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.277       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    value_loss           | 0.717       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.57        |\n",
      "|    ep_rew_mean          | -0.113      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 222         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010906396 |\n",
      "|    clip_fraction        | 0.089       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.117       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.294       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    value_loss           | 0.696       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3.36         |\n",
      "|    ep_rew_mean          | -0.313       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 219          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 46           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072353953 |\n",
      "|    clip_fraction        | 0.0537       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.54        |\n",
      "|    explained_variance   | 0.17         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.287        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00748     |\n",
      "|    value_loss           | 0.697        |\n",
      "------------------------------------------\n",
      "Opponent: random\n",
      "Defeat rate: 20.75%\n",
      "Losses (play first): 95\n",
      "Losses (play second): 320\n",
      "Opponent: smart_random\n",
      "Defeat rate: 98.10%\n",
      "Losses (play first): 1000\n",
      "Losses (play second): 962\n",
      "\u001B[32mSaved new best agent at checkpoint 0\u001B[0m\n",
      "\n",
      "\u001B[33m=== Training segment 2/10 ===\n",
      "Steps: 10000-20000\n",
      "Params: n_steps=2372, batch=665, ent_coef=0.0200\n",
      "Opponents: ['random', 'smart_random']\u001B[0m\n",
      "\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 3.48     |\n",
      "|    ep_rew_mean     | -0.201   |\n",
      "| time/              |          |\n",
      "|    fps             | 266      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.15        |\n",
      "|    ep_rew_mean          | -0.474      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 236         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006540605 |\n",
      "|    clip_fraction        | 0.062       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.245       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.009      |\n",
      "|    value_loss           | 0.615       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 3.23        |\n",
      "|    ep_rew_mean          | -0.364      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 228         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008905065 |\n",
      "|    clip_fraction        | 0.082       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.245       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.236       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 0.579       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 3.31         |\n",
      "|    ep_rew_mean          | -0.241       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 225          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 36           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074272626 |\n",
      "|    clip_fraction        | 0.0598       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.215        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.24         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.0106      |\n",
      "|    value_loss           | 0.63         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Run the training loop\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mmain_training_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[5], line 16\u001B[0m, in \u001B[0;36mmain_training_loop\u001B[0;34m(nb_agents_to_train)\u001B[0m\n\u001B[1;32m     14\u001B[0m trained_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m trained_count \u001B[38;5;241m<\u001B[39m nb_agents_to_train:\n\u001B[0;32m---> 16\u001B[0m     improvement \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_one_agent\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# now returns only improvement\u001B[39;00m\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m improvement:\n\u001B[1;32m     18\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mGREEN\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mTraining completed for agent \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrained_count\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Best agent saved.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mRESET\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[4], line 41\u001B[0m, in \u001B[0;36mtrain_one_agent\u001B[0;34m()\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOpponents: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mopponent_pool\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mRESET\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Train agent\u001B[39;00m\n\u001B[0;32m---> 41\u001B[0m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mCHECKPOINT_INTERVAL\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;66;03m# Evaluate agent\u001B[39;00m\n\u001B[1;32m     44\u001B[0m results \u001B[38;5;241m=\u001B[39m evaluate_agent_by_opponent(agent, opponent_pool, n_episodes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2000\u001B[39m)\n",
      "File \u001B[0;32m~/PROGRAMMATION/PYTHON/PERSO/EXERCISM/venv/lib/python3.10/site-packages/sb3_contrib/ppo_mask/ppo_mask.py:454\u001B[0m, in \u001B[0;36mMaskablePPO.learn\u001B[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, use_masking, progress_bar)\u001B[0m\n\u001B[1;32m    451\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    453\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps \u001B[38;5;241m<\u001B[39m total_timesteps:\n\u001B[0;32m--> 454\u001B[0m     continue_training \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect_rollouts\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrollout_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_steps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_masking\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    456\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m continue_training:\n\u001B[1;32m    457\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/PROGRAMMATION/PYTHON/PERSO/EXERCISM/venv/lib/python3.10/site-packages/sb3_contrib/ppo_mask/ppo_mask.py:230\u001B[0m, in \u001B[0;36mMaskablePPO.collect_rollouts\u001B[0;34m(self, env, callback, rollout_buffer, n_rollout_steps, use_masking)\u001B[0m\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m use_masking:\n\u001B[1;32m    228\u001B[0m         action_masks \u001B[38;5;241m=\u001B[39m get_action_masks(env)\n\u001B[0;32m--> 230\u001B[0m     actions, values, log_probs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction_masks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction_masks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    232\u001B[0m actions \u001B[38;5;241m=\u001B[39m actions\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m    233\u001B[0m new_obs, rewards, dones, infos \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(actions)\n",
      "File \u001B[0;32m~/PROGRAMMATION/PYTHON/PERSO/EXERCISM/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PROGRAMMATION/PYTHON/PERSO/EXERCISM/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/PROGRAMMATION/PYTHON/PERSO/EXERCISM/venv/lib/python3.10/site-packages/sb3_contrib/common/maskable/policies.py:137\u001B[0m, in \u001B[0;36mMaskableActorCriticPolicy.forward\u001B[0;34m(self, obs, deterministic, action_masks)\u001B[0m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;66;03m# Evaluate the values for the given observations\u001B[39;00m\n\u001B[1;32m    136\u001B[0m values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalue_net(latent_vf)\n\u001B[0;32m--> 137\u001B[0m distribution \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_action_dist_from_latent\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlatent_pi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m action_masks \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    139\u001B[0m     distribution\u001B[38;5;241m.\u001B[39mapply_masking(action_masks)\n",
      "File \u001B[0;32m~/PROGRAMMATION/PYTHON/PERSO/EXERCISM/venv/lib/python3.10/site-packages/sb3_contrib/common/maskable/policies.py:250\u001B[0m, in \u001B[0;36mMaskableActorCriticPolicy._get_action_dist_from_latent\u001B[0;34m(self, latent_pi)\u001B[0m\n\u001B[1;32m    243\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    244\u001B[0m \u001B[38;5;124;03mRetrieve action distribution given the latent codes.\u001B[39;00m\n\u001B[1;32m    245\u001B[0m \n\u001B[1;32m    246\u001B[0m \u001B[38;5;124;03m:param latent_pi: Latent code for the actor\u001B[39;00m\n\u001B[1;32m    247\u001B[0m \u001B[38;5;124;03m:return: Action distribution\u001B[39;00m\n\u001B[1;32m    248\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    249\u001B[0m action_logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_net(latent_pi)\n\u001B[0;32m--> 250\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maction_dist\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproba_distribution\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction_logits\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction_logits\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PROGRAMMATION/PYTHON/PERSO/EXERCISM/venv/lib/python3.10/site-packages/sb3_contrib/common/maskable/distributions.py:136\u001B[0m, in \u001B[0;36mMaskableCategoricalDistribution.proba_distribution\u001B[0;34m(self, action_logits)\u001B[0m\n\u001B[1;32m    131\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mproba_distribution\u001B[39m(\n\u001B[1;32m    132\u001B[0m     \u001B[38;5;28mself\u001B[39m: SelfMaskableCategoricalDistribution, action_logits: th\u001B[38;5;241m.\u001B[39mTensor\n\u001B[1;32m    133\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m SelfMaskableCategoricalDistribution:\n\u001B[1;32m    134\u001B[0m     \u001B[38;5;66;03m# Restructure shape to align with logits\u001B[39;00m\n\u001B[1;32m    135\u001B[0m     reshaped_logits \u001B[38;5;241m=\u001B[39m action_logits\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_dim)\n\u001B[0;32m--> 136\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistribution \u001B[38;5;241m=\u001B[39m \u001B[43mMaskableCategorical\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogits\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreshaped_logits\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/PROGRAMMATION/PYTHON/PERSO/EXERCISM/venv/lib/python3.10/site-packages/sb3_contrib/common/maskable/distributions.py:43\u001B[0m, in \u001B[0;36mMaskableCategorical.__init__\u001B[0;34m(self, probs, logits, validate_args, masks)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     37\u001B[0m     probs: Optional[th\u001B[38;5;241m.\u001B[39mTensor] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     40\u001B[0m     masks: MaybeMasks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     41\u001B[0m ):\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmasks: Optional[th\u001B[38;5;241m.\u001B[39mTensor] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m---> 43\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogits\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidate_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     44\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_original_logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogits\n\u001B[1;32m     45\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_masking(masks)\n",
      "File \u001B[0;32m~/PROGRAMMATION/PYTHON/PERSO/EXERCISM/venv/lib/python3.10/site-packages/torch/distributions/categorical.py:66\u001B[0m, in \u001B[0;36mCategorical.__init__\u001B[0;34m(self, probs, logits, validate_args)\u001B[0m\n\u001B[1;32m     64\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`logits` parameter must be at least one-dimensional.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;66;03m# Normalize\u001B[39;00m\n\u001B[0;32m---> 66\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogits \u001B[38;5;241m=\u001B[39m logits \u001B[38;5;241m-\u001B[39m \u001B[43mlogits\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogsumexp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeepdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_param \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprobs \u001B[38;5;28;01mif\u001B[39;00m probs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogits\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_events \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_param\u001B[38;5;241m.\u001B[39msize()[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "defeat_rate_plot()",
   "id": "594c0b9e981ae998",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
